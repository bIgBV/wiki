---
title: Notes for CS491 High Performance Concurrent Systems
---
# Notes

## 11 September 2018
* `H` in perf report annotate to go to the hottest line
* `perf stat` output: `stalled-cycles-frontend` the part which figures out what to run next
* Effects of memory allocation
    * Local variables
        * allocated on the stack or in a register
        * or not at all (with an optimizing compiler)
        * Callee-save registers
             * r8-r15
        * caller-save registers
            * rax-r8
    * Stack
        * local variables
        * good memory locality
            * since your program is always accessing the stack, so it'll be in the cache.
        * Limited storage space.
        * Limited lifetime
    * Heap
        * sbrk-allocated heap, malloc managed
            * high overhead for allocation
            * virtually unlimited size
            * space: min 24 bytes
            * CPU: needto find a chunk
            * freeing a large amount of memory doesn't always work
        * mmap allocated.
        * Dynamically allocate as much as you want.
        * Generally speaking slower because memory locality.
    * Static
        * allocated compile time
        * if initialized they are allocated in the binary.
    * Thread local
        * _thread int count;
        * now there is a count for every thread.
    * speculative execution
        * for each branch, guess which way it will go
            * execute speculatively
            * but don't save the results.

## 13 September 2018

* Speculative execution
* A x86 CPU actually works by translating the CISC instructions on the fly to micro ops (operations) and executing those.
* Store buffers to ensure the result of speculative execution isn't exposed too early and the stores appear to be in order no matter what order they are executed in
* The CPU resources are split in half when hyperthreading.
* a `volatile` variable is always read from memory, and checked to make sure it hasn't changed.

## 25 September 2018

* task parallel programming with pthreads
* threads appear to run continuously.
    * Done through context switching, using timing interrups.
    * With timing interrupts: pre-emtive multitasking
    * The alternate is cooperative multitasking.
    * Generally 10 ms intervals, sometimes 1ms
* `pthread_create(funciton_to_run)`: start a thread.
* pthread_exit(ret_val): finish executing thread. Or just return from the function passed to `pthread_create`.
* `pthread_join(thread_to_join)`: wait until this thread finishes executing.
    * wait until other thread calls pthread_exit.
    * Run 5 things in parallel and wait until they are done executing
    * creates a happens-before-edge
        * This means that anything that happens in the main thread after the join happens after everything that happened in the parallel threads.
* Lock
    * Mutual exclusion. Doesn't enforce an order, but ensure that things don't happen at the same time.
    * acquisition: pthread_mutex_lock/ptrhead_spinlock_lock
        * alternative: ptrhead_mutex_trylock - doesn't block.
    * release: pthread_mutex_unlock/pthread_spinlock_unlock
    * Critical section - a piece of code that runs single-threaded (per lock.)
        * acquire(lock) -> critical section -> release(lock)
    * fine-grained locking, usually preferable to coarse-grained locking.
        * Ex: one big lock on a hash table vs individual locks for each bucket.
* Barrier
    * pthread_barrier_init(size)
    * pthread_barrier_wait()
* Condition variable
    * kinda difficult to use.
    * `pthread_cond_t condvar`
    * `pthread_cond_wait(condvar, lock)`: supposed to wait until you get a signal on the condition variable. You hold the lock when sleeping, but cond wait will ensure that you release that lock when you wake back up.
    * `pthread_cond_signal(condvar)`
    * cond_wait can return spuriously as well
* ad-hoc synchronization.
    * `while(!start)`
    * Kinda sketchy. If `start` is not declared `volatile` the compiler might optimize it away to a `while(true)` and the thread loops forever.


## September 27th

Storage heirarchy in a multi-threaded application. threads running on at least two separate cores and try to access the same piece of memory.

* load virtual_address, register
    * virtual_address -> physical address.
        * address: [page number]:[12 bit page offset]
        * page table: lookup table from virtual page number -> physical page (4096 bytes).
        * Translation Lookaside Buffer: page table cache.
        * Check some caches:
            * L1
            * L2
            * LLC (Last Level Cache)
            * remote LLC
            * DRAM
        * No guarantee that the value is the most recent change.
* store
    * virtual -> physical
    * Example when two processes try writing to one cache line.
    * acquire the cache line in exclusive mode(MOESI)
        * modified: a process has modified the value and the latest value needs to be asked from it.
        * owned: read only but only one process has the copy (probably?)
        * exclusive: before writing to a cache line, only one process can be in exclusive mode. Need to coordinate with other cores/sockets to ensure that.
        * shared: read only, but multiple copies.
        * invalid:
    * example: two stores to different cache lines.
        * on a single CPU it's not really an issue as there is no dependency between the two.
        ```
        data = {some data} // in a remote LLC
        data_is_ready_flat = 1 // in L1
        ```

        Now we have an issue here as the flag will be updated before the data being written to the cache line since write to a remote LLC take time.

    * Memory consistency model:
        * Sequential consistency:
            * If we can interleave the instructions running on multiple cores and run it on a single CPU and get the same results, it's sequential consistency model.
        * On x86: Total store order
            * any stores by a single CPU are visisble to all other CPUs in the same order
            * So in the previous example, it would okay that the data is being written to a remote LLC
            * In order to ensure that, the data doesn't end up on the cache, it stores the value in a store buffer, a FIFO queue.
        * Store buffer:
            * there to provide total store order
            * or to make stores faster
            * 56 entries in recent CPUs
        * Example load after store:
            ```
            mov $5, x
            mov x, %rax
            ```
            * loads from store buffer ~L1 speeds
        * atomic/locked instructions
            * `lock xchg`
            * `lock inc`
            * Get the cacheline in exclusive mode when you read. Then you perform a write, wait for the write to clear out from the store buffer before releasing the cacheline.


# Assignments:

## Assignment 0

Bug fixes:

* Remove unnecessary for loop incrementing volatile `dummy` variable
    * Found out using `qcachegrind` visualizing the large amount of time spent
      in main. Once the hotspot was spotted, I used the `annotate` option in
      `perf report` to find out the offending for loop

* Remove unnecessary call to macro
    ```
    volatile int vec_stat=0; #define
    TriPeVec(a,b,c) for(int i=0;i<1000;i++) vec_stat++;
    ```
    * Found similarly by going through the output in `qcachegrind`.

* Remove unnecessary `sprintf` call.
    * Found by going through hotspots in `perf report`
* Remove unnecessary `mmap` call
    * Found by going through hotspots in `perf report`
* Remove unnecessary `sizeof` call
    * Why was this causing page faults?
* Remove unnecessary sleep
    * bcc tools' `offcputime` is awesome to find out what your program is doing
      when it's not running on the CPU. In this case, after removing all the
      unnecessary functions, the program was still 4 seconds slower than the
      original and this time was not spent on the CPU. It took me a second to
      understand what the output meant, but it shows the kernel stacktrace for
      the offcputime of a program and I could see that the syscall was to
      sleep.

## Assignment 1

### one vs one_opt

**Hypothesis**: 
The compiler just computes the result of adding 1_000_000_000 to 1

**Prediction**:
The assembly will not have a loop in it, instead it will have an add instead

**Discovery**:
The compiler removes everything and just clears `eax` when `-O3` is enabled.

**Test**:
Use the result of the couting somewhere, but don't return anything from the function. Even now, the the time taken is not a lot different that it was previously present. 

**Hypothesis**:
When the internal variable is used in the function, the compiler doesn't eliminate the code, but instead figures out that the loop is simply counting to 1_000_000_000 and uses the `add` instruction.

**Prediction**:
The assembly will not have a jump instruction and instead will have an addition with a constant equalling 1_000_000_001

**Result**
This proves true when looking at the generated assembly. When the local variable is used, the compiler can't simply eliminate the code, so instead it uses an add instruction to add the final result and store it in location of the local variable on the stack and uses that for wherever the value is used.

### two vs two_opt

**Hypothesis**
Similar to one vs opt_one, the compiler removes unnecessary code, and instead computes the final result at compile time and stores it in the global variable

**Prediction**
Changing the inner loop to another computation which updates the value of count will result in compiler just computing the result at compile time and storing that in the global variable.

**Result**
This can be seen in the following screenshots of various expressions and the resulting assembly with the value computed being stored in the global variable.


### three vs three_opt
**Hypothesis**
Due to the `volatile` qualifier, the compiler does not optimize away the loop code.

**Prediction**
If the modificiation to the `volatile` variable is not done in a loop, it will be pre-computed, but if it is a loop it won't be modified.

**Result**
