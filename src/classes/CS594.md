---
title: CS 401
subtitle: >
    A collection of my notes for CS 401 Algorithms
---


# Class notes

* Covering how to make the low level performance of no-SQL databses as fast as possible.
* Read the papers.

# MemC3: Compact concurrent memcache with dumber caching and smarter hashing

An aside: The name is awesome.

* Main improvements: 30% less memory for smaller key-value pairs and 3x as many queries per second over the network.
* Low latency data access means that everything is in Main memory as much as possible.
* Standard memcached is a adjacency list based hash table with a LRU based cache eviction scheme which uses doubly linked lists and locks to safely remove items from the table.
* Specific contributions:
    * Optimistic cuckoo hasing.
    * CLOCK based eviction algorithm which only requires 1 extra bit of information and supports concurrent cache operations.
    * Optimistic locking which reduces inter-thread synchronization.

## Memcached background

* Networked in-memory key-value store
* Chaining adjacency list based hash table.
* Slab based memory allocator:
    * Memory is divided into 1MB pages. Each page is further divided into chunks
    * Different slab classes have different chunk sizes and consequently different number of chunks per page.
    * To insert, memcached looks the slab closest to the key-value being inserted. If a vacant chunk is available, it will use this memory, or else an entry of this size will have to be evicted.
* Each slab maintains it's own LRU queue as a linked list.
* The in-memory data store can be shareded across different cores.

### Real world usecases

* Most keys smaller than 32 bytes and most values only a few hunderd bytes in size.
* Storage overhead for such smalls values is pretty high, as memcached always allocates a 56 byte header for each key-value object despite its size.
    * 2 pointers for the LRU linked list: 16 bytes
    * 1 pointer for the chaining hash table: 8 bytes.
* Queries are read heavy.
* 3 global locks needed to acquire for a single `GET` operation.

## Optimistic concurrent cuckoo hashing

* Basic cuckoo hashing doesn't support concurrent accesss.
* Multiple reader/single writer access
* short summary of each key to improve cache locality of hash table operations.
* optimizations for increased throughput.

This technique has a few advantages:

* 90% space utilization
* Lookups need two parallel cacheline reads followed by up to one memory reference on average.
* Supports multiple readers and single writer.

Basic cuckoo hashing:

* Use two hash functions instead of one => Provides two location for a key.
* Dynamically relocate existing keys and refine the table during insertion.


*Aside*: This is 4-way associative. This is what enables the improved space utilization over standard cuckoo hashing.

* The buckets in this hash table have 4 slots, each containing a pointer to the key-value object and a short summary of the key called a tag.
* The full keys and values are not stored in the hash table, instead only the tag and the pointer are stored there.
* `NULL` indicates an empty slot
* Lookups scan 8 possible locations for a given key
* Inserts try to find an empty slot from the 8 locations. If none is found, then it selects a random key and tries to relocate it to its alternative location. If that's full it tries the same step until either a slot is found, or the max number of displacements is hit. At this point the insertion fails.
* Amortized cost of insertions is `O(1)`


### Tag based lookup/insert

* Use partial cuckoo hashing to generate the 1 byte tag stored in the slot.
* Compute the 1 byte tag and store it with the pointer. Lookups first compare against the tag and only dereference the pointer if the tag matches
* Each bucket fits on a CPU cacheline, each lookup only makes two parallel cacheline-sized reads on average.

* The two buckets are generated by using the key and the tag:
    ```
    b1 = HASH(x)
    b2 = b1 XOR HASH(tag)
    ```
* `b1` can be computed from the same formula using `b2` and the `tag`
* This means that insertions can operate with information only from the table without having to retrieve the key.


### Concurrent access

* Cuckoo path: a sequence of displaced keys for an insert operation.
* Two main issues with concurrency:
    * reader/writer deadlock: You don't know ahead of time which keys will be moved, so you cannot acquire the locks for the path before hand.
    * false misses: when displacing keys in the path, at each step when a key is displaced, it is temprarily not in the table, and if a read occurs then it would be reported a miss, which is false.
* To avoid reader/writer deadlocks, only one writer is permitted at a time.
* False misses are eliminated by:
    * separating the discovery of a valid cuckoo path from the execution of the path
    * moving keys backwards along the path: When a valid path is found, the last displacement will be to an empty slot. The slot and the key are swapped, and then the next to last key is swapped with this empty slot and so on until the first key.

*Optimization: Optimistic locks*

* Single writer to synchronize Insert and lookups
* Instead of locks, we have version counters for each key.
* On insert, the version for a key is updated when it is displaced and lookups check for a version change when a key is hit to check for concurrent displacement.
    * An array of counters: `i-th` counter is for all keys in i-th bucket
    * All counters are initialized to 0 and read/updated by atomic memory operations
* When displacing a key, the insert first increases the relavant counter and then once again after the displacement is completed.
* Before the lookup reads the two buckets, it records the counter values. If the value is odd, there is an insertion occurring on the same key (or another key sharing the same counter). After the lookup, it records the counters again and checks if the versions match. If they don't then a retry occurs.


# Concurrent cache management with CLOCK.

* Space efficient: 1 bit per key for LRU management
* Two operations:
    * `update` to keep track of recency of a key
    * `evict` to select keys to remove when inserting a new key
* In regular memcached Updates to one linked list are serialized.
* Each slab class has a circular buffer and a virtual hand:
    * each bit in the buffer represents the recency of the key-value object.
    * 1 for recently used and 0 otherwise.
    * Update simply sets the bit to 1.
    * Evict checks the bit currently pointed to by the hand. If the bit is 0 it evicts the corresponding key-value object. Otherwise the bit is reset to 0 and the hand is moved forward until we see a 0.

Integrating this with cuckoo hashing:

* The eviction process has to be synchronized with other reader threads
* When evicts finds a key value object to remove from the table, it first increases the key's corresponding counter to indicate an update in progress to other readers. It then removes the key from the table so that it is not accessible in subsequent retries. It then Increases the counter again to complete the change for this key.
* All updates to the counter itself is atomic and synchronized with iserts

# 13 Feb, 2019

* Paper: [OSDI'18] Write-Optimized and High-Performance Hashing Index Scheme for Persistent Memory

# The Adaptive Radix Tree

Binary trees are slow on modern hardware due to poor cache performance. ART provides very efficient indexing as well as insertion and deletion operations. It's performance is comparable to hashtables, but maintains sorted order.

## Adaptive Radix Trees

* Height and time complexity depends on the length of the keys
* No rebalancing required. Insertions done in sorted order.
* Path to a leaf node represents the key of the leaf.

Two types of nodes:
* Inner nodes
* Leaf nodes: Store values.

Efficient way of storing inner nodes is as an array of $2^s$ pointers. An $s$ bit chunk is used as the index into this array during traversal.o

Radix trees have an issue with exponentially increasing space requirements as $s$(size of arrays in inner nodes) is increased. This is because most pointers will be pointing to NULL. ART fixes this by adaptivley using different node types for inner nodes.
